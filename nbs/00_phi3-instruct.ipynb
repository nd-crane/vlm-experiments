{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ph3_instruct\n",
    "\n",
    "> Experiments with loading and evaluating phi-3 instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "python -m mlx_vlm.generate --model mlx-community/Phi-3-vision-128k-instruct-8bit --max-tokens 100 --temp 0.0\n",
    "\n",
    "\n",
    "## Microsoft Phi-3Cookbook\n",
    "\n",
    "[](https://github.com/microsoft/Phi-3CookBook/blob/3f19744fe79582fd85d858895a974a70b254d472/code/09.UpdateSamples/Aug/mlx-phi35-vision.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp phi3_instruct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image loadin from command line\n",
    "\n",
    "```sh\n",
    "cvardema$ python -m mlx_vlm.generate --model mlx-community/Phi-3-vision-128k-instruct-8bit --max-tokens 100 --temp 0.0\n",
    "Fetching 14 files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00<00:00, 80881.90it/s]\n",
    "/Users/cvardema/dev/git/nd-crane/vlm-experiments/.venv/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:513: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
    "  warnings.warn(\n",
    "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
    "==========\n",
    "Image: http://images.cocodataset.org/val2017/000000039769.jpg \n",
    "\n",
    "Prompt: <|user|>\n",
    "<|image_1|>\n",
    "What are these?<|end|>\n",
    "<|assistant|>\n",
    "\n",
    "These are cats lying on a pink surface with remote controls.<|end|>\n",
    "==========\n",
    "Prompt: 4.995 tokens-per-sec\n",
    "Generation: 19.464 tokens-per-sec\n",
    "\n",
    "```\n",
    "\n",
    "![Test Image](./img/000000039769.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe931ed872694152ac107e224ad2322c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlx.core as mx\n",
    "from mlx_vlm import load, generate\n",
    "model_path = \"mlx-community/Phi-3-vision-128k-instruct-8bit\"\n",
    "model, processor = load(model_path,processor_config={\"trust_remote_code\":\"True\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = processor.tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": f\"<image>\\nWhat are these?\"}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests, base64\n",
    "\n",
    "image = Image.open(\"./img/000000039769.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "total images must be the same as the number of image tags, got 0 image tags and 1 images",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m output\n",
      "File \u001b[0;32m~/dev/git/nd-crane/vlm-experiments/.venv/lib/python3.12/site-packages/mlx_vlm/utils.py:908\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model, processor, image, prompt, image_processor, temp, max_tokens, verbose, formatter, repetition_penalty, repetition_context_size, top_p)\u001b[0m\n\u001b[1;32m    905\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer\n\u001b[1;32m    907\u001b[0m image_token_index \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mimage_token_index\n\u001b[0;32m--> 908\u001b[0m input_ids, pixel_values, mask \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_token_index\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    912\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m    913\u001b[0m detokenizer \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mdetokenizer\n",
      "File \u001b[0;32m~/dev/git/nd-crane/vlm-experiments/.venv/lib/python3.12/site-packages/mlx_vlm/utils.py:702\u001b[0m, in \u001b[0;36mprepare_inputs\u001b[0;34m(image_processor, processor, image, prompt, image_token_index)\u001b[0m\n\u001b[1;32m    700\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m mx\u001b[38;5;241m.\u001b[39marray(np\u001b[38;5;241m.\u001b[39mexpand_dims(pixel_values, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 702\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m mx\u001b[38;5;241m.\u001b[39marray(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    704\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m mx\u001b[38;5;241m.\u001b[39marray(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/b9e380c2d981081010843c7e53287c8b827d85be/processing_phi3_v.py:116\u001b[0m, in \u001b[0;36mPhi3VProcessor.__call__\u001b[0;34m(self, text, images, padding, truncation, max_length, return_tensors)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     image_inputs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 116\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_images_texts_to_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/b9e380c2d981081010843c7e53287c8b827d85be/processing_phi3_v.py:174\u001b[0m, in \u001b[0;36mPhi3VProcessor._convert_images_texts_to_inputs\u001b[0;34m(self, images, texts, padding, truncation, max_length, return_tensors)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m unique_image_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(unique_image_ids)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_ids must start from 1, and must be continuous int, e.g. [1, 2, 3], cannot be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munique_image_ids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# total images must be the same as the number of image tags\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_image_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(images), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal images must be the same as the number of image tags, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unique_image_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m image tags and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m image_ids_pad \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m-\u001b[39miid]\u001b[38;5;241m*\u001b[39mnum_img_tokens[iid\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m iid \u001b[38;5;129;01min\u001b[39;00m image_ids]\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minsert_separator\u001b[39m(X, sep_list):\n",
      "\u001b[0;31mAssertionError\u001b[0m: total images must be the same as the number of image tags, got 0 image tags and 1 images"
     ]
    }
   ],
   "source": [
    "output = generate(model, processor, image, prompt, verbose=False)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "                {\"role\": \"user\", \"content\": \"Summarize the video.\"}, \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = processor.tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generate(model, processor, images, placeholder+prompt, verbose=False, max_tokens=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
