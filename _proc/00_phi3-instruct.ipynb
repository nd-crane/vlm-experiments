{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Experiments with loading and evaluating phi-3 instruct\n",
    "output-file: phi3-instruct.html\n",
    "title: Ph3_instruct\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "python -m mlx_vlm.generate --model mlx-community/Phi-3-vision-128k-instruct-8bit --max-tokens 100 --temp 0.0\n",
    "\n",
    "\n",
    "## Microsoft Phi-3Cookbook\n",
    "\n",
    "[](https://github.com/microsoft/Phi-3CookBook/blob/3f19744fe79582fd85d858895a974a70b254d472/code/09.UpdateSamples/Aug/mlx-phi35-vision.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image loadin from command line\n",
    "\n",
    "```sh\n",
    "cvardema$ python -m mlx_vlm.generate --model mlx-community/Phi-3-vision-128k-instruct-8bit --max-tokens 100 --temp 0.0\n",
    "Fetching 14 files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00<00:00, 80881.90it/s]\n",
    "/Users/cvardema/dev/git/nd-crane/vlm-experiments/.venv/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:513: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
    "  warnings.warn(\n",
    "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
    "==========\n",
    "Image: http://images.cocodataset.org/val2017/000000039769.jpg \n",
    "\n",
    "Prompt: <|user|>\n",
    "<|image_1|>\n",
    "What are these?<|end|>\n",
    "<|assistant|>\n",
    "\n",
    "These are cats lying on a pink surface with remote controls.<|end|>\n",
    "==========\n",
    "Prompt: 4.995 tokens-per-sec\n",
    "Generation: 19.464 tokens-per-sec\n",
    "\n",
    "```\n",
    "\n",
    "![Test Image](./img/000000039769.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe931ed872694152ac107e224ad2322c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlx.core as mx\n",
    "from mlx_vlm import load, generate\n",
    "model_path = \"mlx-community/Phi-3-vision-128k-instruct-8bit\"\n",
    "model, processor = load(model_path,processor_config={\"trust_remote_code\":\"True\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests, base64\n",
    "\n",
    "image = Image.open(\"./img/000000039769.jpg\")\n",
    "placeholder = f\"<|image_1|>\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "prompt = processor.tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": f\"<image>\\nWhat are these?\"}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'These are cats lying on a pink surface with remotes beside them.<|end|>'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = generate(model, processor, image, placeholder+prompt, verbose=False)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "                {\"role\": \"user\", \"content\": \"Summarize the video.\"}, \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "prompt = processor.tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "output = generate(model, processor, images, placeholder+prompt, verbose=False, max_tokens=1024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
